Пояснительная записка к тестовому заданию "Прокат автомобилей"


В соотвествие с требованиями было реализовано веб приложение:

В качестве системы хранения выбрана sql БД Postgres. Выбор субд обусловлен тем что в postgres есть механизм событий, с помощью которых все экземпляры приложения получают  информацию об изменении журнала аренды (RENTAL). Также  Postgres умеет шардинг и репликацию.
Приложение получив уведомление от БД отправляет соответвующее сообщение всем подключенным клиентам по протоколу websocket, что отображается на пользовательских страницах.

Для примера использования, в качестве in memory кэша выбран redis. Кэшируются данные которые предоставляют:

* /api/v1/rentjournal
* /api/v1/rentjournal/{rn}
* /api/v1/departments
 
В кэше можно было пременить дублирующие значения для исключения множественного запуска "долгих" запросов к хранилищу от разных клиентских запросов (н-р: Expiration + 10*time.Minute). Также нужно кэшировать статистику по истории, разложив в кэш по ключам  подразделение+типам и подразделение+моделям например: department:model department:type и при вызове /rent /return, а может быть и при событии от хранилица обновлять кэш.

В архитекруре приложение использован шаблон "Репозиторий", что позволило достаточно просто реализвать кэш, используя встраивание:

``` type Storage {}
    func (Storage) GetData{}
    
    type Cache struct {
        Storage
    }
    func (c *Cache) GetData{
        //Если есть кэш, то отдай 
        //Иначе  c.Storage.GetData()
    }
```

Так же использование шаблон "Репозиторий" для хранилища позволило убрать прямые зависимости от роутов и непосредственной реализации хранилища, что в свою очередь позволило тестировать независимо как реализацию хранилища(postgres), так и обработчики api.

Трекинг(/api/v1/track):
По вводным данным 500тыс. ТС  10 раз в минуту отсылают координаты,  что в песиместичном варианте дает 500k rps. В данной реализации приложения узким местом будет вставка БД (тест на i7 c SATA HDD на демо БД дал Requests per second:    564.70 [#/sec] (mean) на трех инстансах приложения). Идея была сделать так: 

Пришедший очередной запрос обрабатываем: по схеме

1. Обрабатываем запрос: Получаем координаты

2. Отправляем в очередь сообщений в виде задания (асинхронно)

2. "Отпускаем" запрос клиента

3. Отдельный(или несколько) воркер отрабатывает задания и непосредственно сохраняет в хранилище

Очередь сообщений можно организовать: RabbitMQ,AMQP,Gearman и т.д.
Реализовать не успел.

В виде эксперемента (отмечено в коде Exp1: Роут: app/routes/trackexp.go): Очередь организовал с каналами непосредственно в приложении. По пяти инстансам (т.е. по 100к входящих запросов) размер в памяти данных займет не много. При тестировании 100к запросов в 10 потоков: Requests per second:   ~10000 [#/sec].  Расход памяти на один контейнер ~77Мб. Естественно фактическая вставка происходила фоном несколько минут.


#### SQL

SQL DDL и DML демо базы находится в каталоге ./sql

#### О странице "История проката транспортных средств":

* Страница отображает историю аренды транспортных средств

* Новые данные истории по аренде загружаеются по websocket

#### О странице "История проката транспортного средства": 

* Страница отображает историю аренды одного транспортного средства

* Страница позволяет выбрать ТС для просмотра

* Страница позволяет выбрать взять в аренду и вернуть ТС

* Новые данные истории по аренде загружаеются по websocket

* на страницу по websocket приходят события из журнала аренды всех ТС, а не только выбранного ТС и фильтруются на клиенте. Оптимальней было бы сделать подписку на определенные ТС для уменьшения трафика и нагрузки

#### О странице "Статистика проката автомобилей в разрезе подразделений"

* Отображение статистики в разрезе моделей ТС
    
* Отображение статистики в разрезе типов ТС

#### Описание API


* GET /api/v1/cars Возвращает список транспортных средств

* GET /api/v1/departments Возвращает список подразделений

* GET /api/v1/car/{rn} Возвращает информацию от ТС с 
рег.номером rn, включая арендовано ли ТС,где, когда и кем

* GET /api/v1/stats/deps/model Возвращает статистику в 
разрезе подраздений и моделей

* GET /api/v1/stats/deps/type Возвращает статистику в 
разрезе подраздений и типов

* POST /api/v1/rent Взять в аренду ТС. Параметры 
формы:regnum;dept;agent

* POST /api/v1/return Вернуть ТС. Параметры формы:regnum;
dept;agent

* GET /api/v1/rentjournal История аренды всех ТС

* GET /api/v1/rentjournal/{rn} История аренды ТС с 
рег.номером rn

* PUT /api/v1/track/{car}/{x}/{y} Отслеживание 
местоположения car-рег.номер. x,y широта и долгота 
соотвественно

* PUT /api/v1/trackexp/{car}/{x}/{y} Эксперимент (очередью в хранилище): Отслеживание местоположения car-рег.номер. x,y широта и долгота соотвественно

Другие роуты:

* /ws websocket

* / Главная страница - "История проката транспортных средств"

* /car История проката транспортного средства с выбором

* /car/{rn} История проката транспортного средства с рег.номером

* /stats Статистика по ТС

#### Запуск приложения (демо)

##### Условия: 



* golang 1.8 или выше

* docker 

* docker-compose

##### Действия:

* ```go get -uv github.com/arteev/er-task```

* в каталоге приложения $GOPATH/arteev/er-task: ```make```

* ```./demo```

При запуске демо должно произойти: 

1. Запуск docker-compose -f demo.yml up

2. Сбор docker образа приложения на основе Dockerfile

3. Запустяться контейнеры образов: postgres,redis,dockercloud/haproxy и образ веб приложения (3 контейнера)

4. Должны на БД postgres накатиться скрипты createdb.sql db.dump example.db.sql

5. Приложение будет доступно на данном хосте по порту 80


Для изменения количества контейнеров приложения необходимо выполнить команду:

```
    docker-compose -f demo.yml scale web=5
```

или изменить в файле demo.sh

Если приложение отвечает 503, то скорее всего необходимо 1 минуту подождать и обновить страницу (не смогло к БД подключиться т.к. скрипты накатывались позднее старта контейнеров)

#### О тестах

* Для прохождения тестов а также сборки с помощью make (make test) необходим docker для запуска контейнера(postgres) с СУБД Postgres

* модуль storage_pg.go необходимо покрыть тестами для проверки уведомлений: добавить абстракцию в виде интерфейса между storagePG и pq.Listener

* Некоторые функции не покрыты тестами из-за экономии времени: н-р: routes/statsdep.go и т.д.

#### О скриптах на клиенте:

* нет юнит тестирования скриптов javascript

* есть дублирование history.js и car.js

* js фактически не рефакторил

#### О комментариях

* Следовало бы на одном языке их делать

* Местами отсутствуют, линтеру не нравится.